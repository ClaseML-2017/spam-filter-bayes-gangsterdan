{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.stats import norm\n",
    "from sklearn import preprocessing\n",
    "from decimal import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/spambase.data\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "          ...                48           49           50           51  \\\n",
       "count     ...       4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      ...          0.038575     0.139030     0.016976     0.269071   \n",
       "std       ...          0.243471     0.270355     0.109394     0.815672   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.065000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.188000     0.000000     0.315000   \n",
       "max       ...          4.385000     9.752000     4.081000    32.478000   \n",
       "\n",
       "                52           53           54           55            56  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   4601.000000   \n",
       "mean      0.075811     0.044238     5.191515    52.172789    283.289285   \n",
       "std       0.245882     0.429342    31.729449   194.891310    606.347851   \n",
       "min       0.000000     0.000000     1.000000     1.000000      1.000000   \n",
       "25%       0.000000     0.000000     1.588000     6.000000     35.000000   \n",
       "50%       0.000000     0.000000     2.276000    15.000000     95.000000   \n",
       "75%       0.052000     0.000000     3.706000    43.000000    266.000000   \n",
       "max       6.003000    19.829000  1102.500000  9989.000000  15841.000000   \n",
       "\n",
       "                57  \n",
       "count  4601.000000  \n",
       "mean      0.394045  \n",
       "std       0.488698  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df[df.columns[0:-1]],df[df.columns[-1]], train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizando los datos para que podamos comparar el modelo con el de sklearn\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train=scaler.transform(X_train)\n",
    "X_test=scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las medias para cada una de las *features* en caso de que el mail sea **spam** (y claro, el complemento son las medias para **not spam**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14837026, -0.02898476,  0.22825946,  0.06238555,  0.31340404,\n",
       "        0.29549304,  0.41099132,  0.25629488,  0.30077684,  0.16996238,\n",
       "        0.31554383, -0.01428414,  0.16939191,  0.06546019,  0.25753277,\n",
       "        0.34809556,  0.3214384 ,  0.24857024,  0.31846277,  0.22992161,\n",
       "        0.46084965,  0.12333631,  0.41626538,  0.24206357, -0.31242658,\n",
       "       -0.29222174, -0.22294198, -0.1852906 , -0.15498299, -0.21381532,\n",
       "       -0.14870974, -0.13486089, -0.14197033, -0.1335133 , -0.16998021,\n",
       "       -0.16904767, -0.20869868, -0.03041117, -0.15317914, -0.06708448,\n",
       "       -0.11535621, -0.1635194 , -0.16154775, -0.11714789, -0.17900607,\n",
       "       -0.18182567, -0.05218251, -0.10418774, -0.06904635, -0.0892024 ,\n",
       "       -0.07790028,  0.26795845,  0.40210014,  0.07128149,  0.13653278,\n",
       "        0.25063978,  0.33617743])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMeanS = np.mean(X_train[Y_train==1], axis = 0)\n",
    "trainMeanNS = 1-trainMeanS\n",
    "trainMeanS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las desviaciones estandar para cada una de las features dependiendo si es **spam** o no. (similar al paso anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.00846045,  0.29292342,  0.94819293,  1.59520211,  1.07274225,\n",
       "        1.22804137,  1.46431414,  1.37720263,  1.27895152,  0.96334644,\n",
       "        1.28088259,  0.72962921,  1.19057065,  0.88124706,  1.51128746,\n",
       "        1.26834123,  1.3947645 ,  1.24018414,  0.87196905,  1.55332522,\n",
       "        1.03122099,  1.43654265,  1.48542926,  1.31896461,  0.10648388,\n",
       "        0.13422074,  0.00347998,  0.68611983,  0.01228859,  0.23097002,\n",
       "        0.10465468,  0.06039775,  0.2017066 ,  0.12863359,  0.13805718,\n",
       "        0.36506022,  0.7045567 ,  0.29050118,  0.19712341,  0.47460948,\n",
       "        0.00740537,  0.04064948,  0.22336628,  0.08845203,  0.34210907,\n",
       "        0.10423242,  0.24657348,  0.0647921 ,  0.43720838,  1.11392491,\n",
       "        0.41501631,  0.8397944 ,  1.46281974,  1.37646848,  1.56831952,\n",
       "        1.54485701,  1.34402963])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSdS = np.std(X_train[Y_train==1], axis = 0)\n",
    "trainSdNS = 1-trainSdS\n",
    "trainSdS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las medias generales de las *features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainMeanG = np.mean(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos las desviaciones estandar generales para cada una de las *features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSdG = np.std(X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos la probabilidad frecuentista de que un mail sea **spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3918840579710145"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pspam = np.mean(Y_train, axis = 0)\n",
    "pspam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos una lista con las tuplas de los parametros de las *features* para las distribuciones normales para los valores de los mails **spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.14837026231587547, 1.008460454246616),\n",
       " (-0.028984755836337001, 0.292923417861844),\n",
       " (0.22825946405321568, 0.94819292771022579),\n",
       " (0.062385553839736707, 1.5952021132069012),\n",
       " (0.31340403745592699, 1.0727422519527208),\n",
       " (0.29549303841533897, 1.2280413677025337),\n",
       " (0.41099131607981998, 1.4643141421102974),\n",
       " (0.2562948848422551, 1.3772026292371531),\n",
       " (0.30077684167609559, 1.2789515225210144),\n",
       " (0.16996238137465738, 0.96334643779932472),\n",
       " (0.31554382663153724, 1.2808825899444931),\n",
       " (-0.014284142567096453, 0.729629208046919),\n",
       " (0.16939190947297586, 1.1905706465605974),\n",
       " (0.065460185623440872, 0.88124706464830593),\n",
       " (0.25753277086189225, 1.5112874617500456),\n",
       " (0.34809555710324053, 1.2683412282903601),\n",
       " (0.3214384012791176, 1.3947645019475234),\n",
       " (0.24857024400455788, 1.2401841387462778),\n",
       " (0.31846276557344572, 0.87196904671109055),\n",
       " (0.22992161348324716, 1.5533252248225093),\n",
       " (0.46084965050381244, 1.0312209894732889),\n",
       " (0.12333631180082426, 1.436542648725758),\n",
       " (0.41626537822782805, 1.485429262019335),\n",
       " (0.24206356682231586, 1.3189646136552036),\n",
       " (-0.31242658067166024, 0.10648388254934639),\n",
       " (-0.2922217419891644, 0.13422073870098197),\n",
       " (-0.22294198445938795, 0.0034799814045915162),\n",
       " (-0.18529060119779062, 0.68611983325692338),\n",
       " (-0.15498299493423956, 0.012288586608086178),\n",
       " (-0.21381532014028332, 0.23097001923939145),\n",
       " (-0.14870974449011432, 0.10465468285731108),\n",
       " (-0.13486088849585895, 0.060397754733961889),\n",
       " (-0.14197032566433707, 0.20170660444094901),\n",
       " (-0.13351329953865121, 0.128633589019228),\n",
       " (-0.16998021097415483, 0.13805717757171973),\n",
       " (-0.16904766859596462, 0.36506022409954403),\n",
       " (-0.20869867593893171, 0.70455670207556276),\n",
       " (-0.03041117339799581, 0.29050118237665878),\n",
       " (-0.15317914304027724, 0.19712340599308698),\n",
       " (-0.067084483689470453, 0.47460948327443664),\n",
       " (-0.1153562092340842, 0.0074053655794084185),\n",
       " (-0.16351939921215103, 0.040649475826812317),\n",
       " (-0.16154775239195082, 0.2233662769043952),\n",
       " (-0.11714789438932591, 0.088452028719493009),\n",
       " (-0.17900606918675213, 0.3421090696845574),\n",
       " (-0.18182566830471836, 0.10423242441028306),\n",
       " (-0.052182507433665995, 0.24657348452218605),\n",
       " (-0.10418774304286742, 0.064792100578100725),\n",
       " (-0.069046349118519951, 0.4372083811399855),\n",
       " (-0.08920240257570182, 1.1139249063908621),\n",
       " (-0.07790028072894796, 0.41501630832433017),\n",
       " (0.26795844946873953, 0.83979439806772693),\n",
       " (0.40210014008032396, 1.4628197405557846),\n",
       " (0.071281485606587591, 1.3764684792681379),\n",
       " (0.13653277753511475, 1.5683195235680596),\n",
       " (0.25063978450910546, 1.5448570147739478),\n",
       " (0.33617742761002162, 1.3440296262570572)]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribSpam = []\n",
    "\n",
    "for i in range(len(trainMeanS)):\n",
    "    distribSpam.append((trainMeanS[i],trainSdS[i]))\n",
    "\n",
    "distribSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos los parametros para las distribuciones de lo **no spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.85162973768412453, -0.0084604542466160115),\n",
       " (1.028984755836337, 0.70707658213815594),\n",
       " (0.77174053594678438, 0.051807072289774214),\n",
       " (0.93761444616026335, -0.5952021132069012),\n",
       " (0.68659596254407296, -0.072742251952720771),\n",
       " (0.70450696158466108, -0.22804136770253369),\n",
       " (0.58900868392018002, -0.46431414211029742),\n",
       " (0.74370511515774496, -0.37720262923715309),\n",
       " (0.69922315832390436, -0.27895152252101441),\n",
       " (0.83003761862534264, 0.036653562200675283),\n",
       " (0.68445617336846276, -0.28088258994449311),\n",
       " (1.0142841425670965, 0.270370791953081),\n",
       " (0.83060809052702411, -0.19057064656059741),\n",
       " (0.93453981437655909, 0.11875293535169407),\n",
       " (0.74246722913810781, -0.51128746175004558),\n",
       " (0.65190444289675953, -0.26834122829036011),\n",
       " (0.6785615987208824, -0.39476450194752344),\n",
       " (0.75142975599544215, -0.24018413874627775),\n",
       " (0.68153723442655423, 0.12803095328890945),\n",
       " (0.77007838651675287, -0.55332522482250934),\n",
       " (0.53915034949618756, -0.031220989473288885),\n",
       " (0.87666368819917573, -0.43654264872575799),\n",
       " (0.58373462177217195, -0.48542926201933501),\n",
       " (0.75793643317768411, -0.31896461365520357),\n",
       " (1.3124265806716602, 0.89351611745065362),\n",
       " (1.2922217419891644, 0.86577926129901805),\n",
       " (1.2229419844593878, 0.99652001859540851),\n",
       " (1.1852906011977906, 0.31388016674307662),\n",
       " (1.1549829949342396, 0.98771141339191382),\n",
       " (1.2138153201402833, 0.76902998076060858),\n",
       " (1.1487097444901142, 0.89534531714268895),\n",
       " (1.1348608884958589, 0.93960224526603808),\n",
       " (1.141970325664337, 0.79829339555905099),\n",
       " (1.1335132995386512, 0.87136641098077194),\n",
       " (1.1699802109741548, 0.86194282242828024),\n",
       " (1.1690476685959645, 0.63493977590045603),\n",
       " (1.2086986759389318, 0.29544329792443724),\n",
       " (1.0304111733979957, 0.70949881762334122),\n",
       " (1.1531791430402771, 0.80287659400691302),\n",
       " (1.0670844836894704, 0.52539051672556336),\n",
       " (1.1153562092340843, 0.99259463442059159),\n",
       " (1.1635193992121511, 0.95935052417318767),\n",
       " (1.1615477523919508, 0.77663372309560486),\n",
       " (1.1171478943893258, 0.91154797128050702),\n",
       " (1.1790060691867521, 0.65789093031544255),\n",
       " (1.1818256683047184, 0.89576757558971698),\n",
       " (1.0521825074336659, 0.75342651547781392),\n",
       " (1.1041877430428675, 0.9352078994218993),\n",
       " (1.06904634911852, 0.5627916188600145),\n",
       " (1.0892024025757019, -0.11392490639086206),\n",
       " (1.0779002807289479, 0.58498369167566988),\n",
       " (0.73204155053126052, 0.16020560193227307),\n",
       " (0.59789985991967609, -0.46281974055578456),\n",
       " (0.92871851439341246, -0.37646847926813787),\n",
       " (0.86346722246488528, -0.56831952356805959),\n",
       " (0.74936021549089449, -0.54485701477394777),\n",
       " (0.66382257238997844, -0.34402962625705724)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribNoSpam = []\n",
    "\n",
    "for i in range(len(trainMeanNS)):\n",
    "    distribNoSpam.append((trainMeanNS[i],trainSdNS[i]))\n",
    "\n",
    "distribNoSpam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos una lista con las tuplas de los parametros de las *features* para las distribuciones normales para los valores de los mails **NO spam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7.131171659621295e-17, 1.0),\n",
       " (-8.238176646493915e-18, 1.0),\n",
       " (-9.7828347677115247e-18, 1.0),\n",
       " (-2.4714529939481747e-17, 0.99999999999999989),\n",
       " (-5.1488604040586971e-17, 1.0),\n",
       " (-1.2872151010146743e-17, 1.0),\n",
       " (7.1569159616415893e-17, 1.0),\n",
       " (0.0, 1.0),\n",
       " (4.7884401757745882e-17, 1.0),\n",
       " (-2.5744302020293485e-17, 0.99999999999999989),\n",
       " (1.7506125373799569e-17, 1.0),\n",
       " (8.2381766464939153e-17, 0.99999999999999989),\n",
       " (-1.4416809131364352e-17, 0.99999999999999989),\n",
       " (3.7071794909222621e-17, 0.99999999999999989),\n",
       " (3.6042022828410876e-17, 1.0),\n",
       " (-2.6774074101105224e-17, 1.0),\n",
       " (4.3765313434498927e-17, 1.0),\n",
       " (5.0458831959775232e-17, 1.0),\n",
       " (2.0904373240478309e-16, 1.0),\n",
       " (-5.3548148202210448e-17, 1.0),\n",
       " (4.4280199474904793e-17, 1.0),\n",
       " (1.2357264969740873e-17, 1.0),\n",
       " (5.6637464444645665e-18, 0.99999999999999989),\n",
       " (-3.5012250747599138e-17, 1.0),\n",
       " (-6.1786324848704366e-18, 1.0),\n",
       " (-1.647635329298783e-17, 1.0),\n",
       " (-2.4714529939481747e-17, 0.99999999999999978),\n",
       " (4.7369515717340015e-17, 1.0),\n",
       " (-1.2872151010146743e-17, 1.0),\n",
       " (2.0595441616234787e-18, 0.99999999999999989),\n",
       " (-2.1625213697046527e-17, 0.99999999999999989),\n",
       " (-2.0595441616234788e-17, 1.0),\n",
       " (-8.7530626868997844e-18, 1.0),\n",
       " (1.1327492888929133e-17, 0.99999999999999989),\n",
       " (3.0893162424352183e-18, 0.99999999999999989),\n",
       " (-2.7803846181916966e-17, 0.99999999999999989),\n",
       " (7.0024501495198275e-17, 1.0),\n",
       " (-1.2357264969740873e-17, 0.99999999999999978),\n",
       " (-3.295270658597566e-17, 1.0),\n",
       " (1.0297720808117394e-17, 1.0),\n",
       " (-8.238176646493915e-18, 1.0),\n",
       " (0.0, 1.0),\n",
       " (3.6042022828410876e-17, 1.0),\n",
       " (-1.0297720808117394e-17, 0.99999999999999989),\n",
       " (5.5607692363833932e-17, 0.99999999999999989),\n",
       " (-2.8061289202119896e-17, 1.0),\n",
       " (1.0297720808117394e-18, 0.99999999999999989),\n",
       " (-2.0595441616234787e-18, 1.0),\n",
       " (2.7288960141511094e-17, 1.0),\n",
       " (-1.647635329298783e-17, 1.0),\n",
       " (1.0297720808117394e-18, 1.0),\n",
       " (3.0893162424352182e-17, 1.0),\n",
       " (-2.3684757858670008e-17, 1.0),\n",
       " (8.238176646493915e-18, 0.99999999999999989),\n",
       " (-1.647635329298783e-17, 1.0),\n",
       " (0.0, 1.0),\n",
       " (2.7803846181916966e-17, 0.99999999999999989)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribGeneral = []\n",
    "\n",
    "for i in range(len(trainMeanG)):\n",
    "    distribGeneral.append((trainMeanG[i],trainSdG[i]))\n",
    "\n",
    "distribGeneral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construccion de una funcion para evaluar cada correo ahora que tenemos todas las herramientas para calcular la probabilidad de que sea spam.\n",
    "\n",
    "P(S|X1,...,X57) = **(**P(S)P(X1|S)...P(X57|S)**)**/P(X1,...,X57)\n",
    "\n",
    "Ahora, los calculos se pueden simplificar si formulamos una regla de clasificacion en la cual la probabilidad de que sea spam (dados los parametros) sea mayor que la probabilidad de que no sea spam (dados los parametros).\n",
    "Ademas, tomando logaritmos los calculos de productos se hacen como sumas, lo cual permite retener precision que se perderia con numeros de punto flotante.\n",
    "\n",
    "ln(P(S)) + ln(P(X1|S)) + ... + ln(X57|S) >= ln(P(~S)) + ln(P(X1|~S)) + ... + ln(X57|~S)\n",
    "\n",
    "Podemos ignorar en la suma probabilidades que son practicamente cero al igual que desviaciones estandar que son tambien muy cercanas a cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nueva funcion para clasificar de manera mas simple\n",
    "def classify(mail):\n",
    "    myZero = 0.0000000000000000001\n",
    "    left = np.log(pspam)\n",
    "    right = np.log(1-pspam)\n",
    "    \n",
    "    for i in range(len(mail)):\n",
    "        #print i\n",
    "        l = norm.pdf(mail[i],loc = distribSpam[i][0],scale = distribSpam[i][1])\n",
    "        r = norm.pdf(mail[i],loc = distribNoSpam[i][0],scale = distribNoSpam[i][1])\n",
    "        if (l > myZero or distribSpam[i][1] > myZero):\n",
    "            left += np.log(l)\n",
    "        if(r > myZero or distribNoSpam[i][1] > myZero):\n",
    "            right += np.log(r)\n",
    "    \n",
    "    if (left >= right):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        if (l > myZero or distribSpam[i][1] > myZero):\n",
    "            left += np.log(l)\n",
    "        if(r > myZero or distribNoSpam[i][1] > myZero):\n",
    "            right += np.log(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#regresar la prediccion armada\n",
    "def predictSpam(test):\n",
    "    res = []\n",
    "    for x in test:\n",
    "        res.append(classify(x))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielespinosa/gitsome/claseMachineLearning/lib/python2.7/site-packages/ipykernel_launcher.py:12: RuntimeWarning: divide by zero encountered in log\n",
      "  if sys.path[0] == '':\n",
      "/Users/danielespinosa/gitsome/claseMachineLearning/lib/python2.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "prediction = predictSpam(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[331, 359],\n",
       "       [  5, 456]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_test,prediction)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
